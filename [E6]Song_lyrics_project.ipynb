{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "essential-wrapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['THE QUEEN _of_ HEARTS', '', '']\n"
     ]
    }
   ],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf    # 대망의 텐서플로우!\n",
    "import os\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-dimension",
   "metadata": {},
   "source": [
    "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.\n",
    "\n",
    "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 너무 긴 문장은 노래가사 작사하기에 어울리지 않을수도 있겠죠.\n",
    "그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외하기를 권합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "certified-pavilion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<start> simple simon <end>',\n",
       " '<start> toad and frog <end>',\n",
       " '<start> sat in a corner <end>',\n",
       " '<start> the wooing <end>',\n",
       " '<start> handy pandy <end>',\n",
       " '<start> blow wind blow <end>',\n",
       " '<start> blue bell boy <end>',\n",
       " '<start> buz and hum <end>',\n",
       " '<start> and so do we . <end>',\n",
       " '<start> else it was he . <end>',\n",
       " '<start> a and b and see <end>',\n",
       " '<start> doctor foster <end>',\n",
       " '<start> queen anne <end>',\n",
       " '<start> ho my kitten <end>',\n",
       " '<start> lavender blue <end>',\n",
       " '<start> cat and dog <end>',\n",
       " '<start> bobby shaft <end>',\n",
       " '<start> little maid <end>',\n",
       " '<start> bat , bat <end>',\n",
       " '<start> bat , bat , <end>',\n",
       " '<start> christmas <end>',\n",
       " '<start> peter white <end>',\n",
       " '<start> up pippen hill <end>',\n",
       " '<start> a falling out <end>',\n",
       " '<start> peg <end>',\n",
       " '<start> georgy porgy <end>',\n",
       " '<start> to market <end>',\n",
       " '<start> to buy a fat pig <end>',\n",
       " '<start> jiggety jig . <end>',\n",
       " '<start> to buy a fat hog <end>',\n",
       " '<start> jiggety jog . <end>',\n",
       " '<start> pancake day <end>',\n",
       " '<start> hush a bye baby <end>',\n",
       " '<start> robin a bobin <end>',\n",
       " '<start> robin a bobin <end>',\n",
       " '<start> bent his bow , <end>',\n",
       " '<start> bandy legs <end>',\n",
       " '<start> a apple pie <end>',\n",
       " '<start> a apple pie <end>',\n",
       " '<start>  <end>',\n",
       " '<start> a b <end>',\n",
       " '<start> was an bit <end>',\n",
       " '<start> apple pie . it . <end>',\n",
       " '<start>  <end>',\n",
       " '<start> c d <end>',\n",
       " '<start> cut dealt <end>',\n",
       " '<start> it . it . <end>',\n",
       " '<start>  <end>',\n",
       " '<start> e f <end>',\n",
       " '<start> eat fought <end>',\n",
       " '<start> it . for it . <end>',\n",
       " '<start>  <end>',\n",
       " '<start> g h <end>',\n",
       " '<start> got had <end>',\n",
       " '<start> it . it . <end>',\n",
       " '<start>  <end>',\n",
       " '<start> j k <end>',\n",
       " '<start> joined kept <end>',\n",
       " '<start> it . it . <end>',\n",
       " '<start>  <end>',\n",
       " '<start> l m <end>',\n",
       " '<start> for it . for it . <end>',\n",
       " '<start>  <end>',\n",
       " '<start> n o <end>',\n",
       " '<start> nodded opened <end>',\n",
       " '<start> for it . it . <end>',\n",
       " '<start>  <end>',\n",
       " '<start> p q <end>',\n",
       " '<start> in it . it . <end>',\n",
       " '<start>  <end>',\n",
       " '<start> r s <end>',\n",
       " '<start> ran stole <end>',\n",
       " '<start> for it . it . <end>',\n",
       " '<start>  <end>',\n",
       " '<start> t v <end>',\n",
       " '<start> took viewed <end>',\n",
       " '<start> it . it . <end>',\n",
       " '<start>  <end>',\n",
       " '<start> w x y <end>',\n",
       " '<start> wanted and z <end>',\n",
       " '<start> it . <end>',\n",
       " '<start>  <end>',\n",
       " '<start> daddy is near <end>',\n",
       " '<start> snail <end>',\n",
       " '<start> my lady wind <end>',\n",
       " '<start> poor robin <end>',\n",
       " '<start> poor thing ! <end>',\n",
       " '<start> poor thing ! <end>',\n",
       " '<start> pussy cat <end>',\n",
       " '<start> of washing <end>',\n",
       " '<start> half a crown <end>',\n",
       " '<start> the rose is red <end>',\n",
       " '<start> the wind <end>',\n",
       " '<start> a warning <end>',\n",
       " '<start> cock crow <end>',\n",
       " '<start> my maid mary <end>',\n",
       " '<start> robin and wren <end>',\n",
       " '<start> humpty dumpty <end>',\n",
       " '<start> a medley <end>',\n",
       " '<start> to the birds <end>',\n",
       " '<start> for if you do , <end>',\n",
       " '<start> dame trot <end>',\n",
       " '<start> if <end>',\n",
       " '<start> how do you do ? <end>',\n",
       " '<start> old king cole <end>',\n",
       " '<start> as can compare <end>',\n",
       " '<start> with king cole <end>',\n",
       " '<start> early rising <end>',\n",
       " '<start> buttons <end>',\n",
       " '<start> sulky sue <end>',\n",
       " '<start> tit tat toe <end>',\n",
       " '<start> tit tat toe , <end>',\n",
       " '<start> my first go , <end>',\n",
       " '<start> all of a row <end>',\n",
       " '<start> stick one up , <end>',\n",
       " '<start> caesar s song <end>',\n",
       " '<start> bow , wow , wow . <end>',\n",
       " '<start> green gravel <end>',\n",
       " '<start> ten fingers <end>',\n",
       " '<start> of pigs <end>',\n",
       " '<start> this is the end <end>',\n",
       " '<start> cross patch <end>',\n",
       " '<start> yankee doodle <end>',\n",
       " '<start> boys and girls <end>',\n",
       " '<start> sing ivy <end>',\n",
       " '<start> pussycat mew <end>',\n",
       " '<start> and that s half <end>',\n",
       " '<start> and that s all . <end>',\n",
       " '<start> seeking a wife <end>',\n",
       " '<start> r , s , t , and u , <end>',\n",
       " '<start> w , x , y , and z . <end>',\n",
       " '<start> of arithmetic <end>',\n",
       " '<start> a varied song <end>',\n",
       " '<start> rub a dub dub , <end>',\n",
       " '<start> for every evil <end>',\n",
       " '<start> andrew <end>',\n",
       " '<start> mary s canary <end>',\n",
       " '<start> the cuckoo <end>',\n",
       " '<start> in april , <end>',\n",
       " '<start> come he will . <end>',\n",
       " '<start> in may , <end>',\n",
       " '<start> in june , <end>',\n",
       " '<start> in july , <end>',\n",
       " '<start> in august , <end>',\n",
       " '<start> go he must . <end>',\n",
       " '<start> a swarm of bees <end>',\n",
       " '<start> i said the fly <end>',\n",
       " '<start> i saw him die . <end>',\n",
       " '<start> i said the fish <end>',\n",
       " '<start> i said the lark <end>',\n",
       " '<start> i said the owl <end>',\n",
       " '<start> i said the rook <end>',\n",
       " '<start> i said the dove <end>',\n",
       " '<start> i said the kite <end>',\n",
       " '<start> i said the bull <end>',\n",
       " '<start> nothing at all <end>',\n",
       " '<start> little bo peep <end>',\n",
       " '<start> to bed ! <end>',\n",
       " '<start> of going to bed <end>',\n",
       " '<start> a golden purse <end>',\n",
       " '<start> a golden bird . <end>',\n",
       " '<start> monday s child <end>',\n",
       " '<start> jack and jill <end>',\n",
       " '<start> the piper s cow <end>',\n",
       " '<start> shave a pig <end>',\n",
       " '<start> tongs <end>',\n",
       " '<start> curly locks <end>',\n",
       " '<start> punch and judy <end>',\n",
       " '<start> punch and judy <end>',\n",
       " '<start> sixpence . <end>',\n",
       " '<start> the burny bee <end>',\n",
       " '<start> danty baby <end>',\n",
       " '<start> tommy s cake <end>',\n",
       " '<start> baker s man ! <end>',\n",
       " '<start> cushy cow <end>',\n",
       " '<start> tell tale tit <end>',\n",
       " '<start> tell tale tit , <end>',\n",
       " '<start> baked in a pie . <end>',\n",
       " '<start> mother goose <end>',\n",
       " '<start> grew very fond <end>',\n",
       " '<start> a lady so gay , <end>',\n",
       " '<start> dear , dear ! <end>',\n",
       " '<start> one came down , <end>',\n",
       " '<start> the nut tree <end>',\n",
       " '<start> brian o lin <end>',\n",
       " '<start> margery daw <end>',\n",
       " '<start> nonsense <end>',\n",
       " '<start> is a good girl , <end>',\n",
       " '<start> lucy locket <end>',\n",
       " '<start> lucy locket <end>',\n",
       " '<start> kitty fisher <end>',\n",
       " '<start> found it <end>',\n",
       " '<start> nothing in it , <end>',\n",
       " '<start> nothing in it , <end>',\n",
       " '<start> round it . <end>',\n",
       " '<start> baby bunting <end>',\n",
       " '<start> dickory , <end>',\n",
       " '<start> dickory , <end>',\n",
       " '<start> dock ! <end>',\n",
       " '<start> dickory , <end>',\n",
       " '<start> dickory , <end>',\n",
       " '<start> dock ! <end>',\n",
       " '<start> a carrion crow <end>',\n",
       " '<start> and a red nose <end>',\n",
       " '<start> one , two . <end>',\n",
       " '<start> one , two , <end>',\n",
       " '<start> buckle my shoe <end>',\n",
       " '<start> three , four , <end>',\n",
       " '<start> five , six , <end>',\n",
       " '<start> pick up sticks <end>',\n",
       " '<start> seven , eight , <end>',\n",
       " '<start> nine , ten , <end>',\n",
       " '<start> a good fat hen <end>',\n",
       " '<start> who will delve <end>',\n",
       " '<start> mary , mary <end>',\n",
       " '<start> silver bells , <end>',\n",
       " '<start> all of a row . <end>',\n",
       " '<start> jack jingle <end>',\n",
       " '<start> what care i ? <end>',\n",
       " '<start> do you not hear <end>',\n",
       " '<start> feetikins <end>',\n",
       " '<start> doctor fell <end>',\n",
       " '<start> i see no reason <end>',\n",
       " '<start> guy , guy , guy , <end>',\n",
       " '<start> billy , billy <end>',\n",
       " '<start> two at my head , <end>',\n",
       " '<start> two at my feet , <end>',\n",
       " '<start> one at my heart <end>',\n",
       " '<start> johnny <end>',\n",
       " '<start> sing , sing ! <end>',\n",
       " '<start> peter piper <end>',\n",
       " '<start> nancy dawson <end>',\n",
       " '<start> london bridge <end>',\n",
       " '<start> master i have <end>',\n",
       " '<start> rock a by , baby <end>',\n",
       " '<start> i ll try <end>',\n",
       " '<start> if <end>',\n",
       " '<start> coffee and tea <end>',\n",
       " '<start> i met a pig <end>',\n",
       " '<start> without a wig , <end>',\n",
       " '<start> my boy tammie <end>',\n",
       " '<start> my boy tammie ? <end>',\n",
       " '<start> my boy tammie ? <end>',\n",
       " '<start> my boy tammie ? <end>',\n",
       " '<start> my boy tammie ? <end>',\n",
       " '<start> clap handies <end>',\n",
       " '<start> there was a man <end>',\n",
       " '<start> jack s fiddle <end>',\n",
       " '<start> a was an archer <end>',\n",
       " '<start> three ships <end>',\n",
       " '<start> a , b , c <end>',\n",
       " '<start> comical folk <end>',\n",
       " '<start> bobby snooks <end>',\n",
       " '<start> the hart <end>',\n",
       " '<start> see , see ! <end>',\n",
       " '<start> to buy him a hat <end>',\n",
       " '<start> to buy him a wig <end>',\n",
       " '<start> to babylon <end>',\n",
       " '<start> my black hen <end>',\n",
       " '<start> another one <end>',\n",
       " '<start> like the jets <end>',\n",
       " '<start> like the jets <end>',\n",
       " '<start> another one ! <end>',\n",
       " '<start> i m the one <end>',\n",
       " '<start> i m the only one <end>',\n",
       " '<start> i m the one <end>',\n",
       " '<start> i m the only one <end>',\n",
       " '<start> i m the one <end>',\n",
       " '<start> i m the only one <end>',\n",
       " '<start> i m the one <end>',\n",
       " '<start> i m the only one <end>',\n",
       " '<start> i m the one <end>',\n",
       " '<start> i m the only one <end>',\n",
       " '<start> i m the one <end>',\n",
       " '<start> roll my eyes <end>',\n",
       " '<start> god forgive me <end>',\n",
       " '<start> i m the one <end>',\n",
       " '<start> i m the only one <end>',\n",
       " '<start> i m the one <end>',\n",
       " '<start> all we do is win <end>',\n",
       " '<start> i m a made nigga <end>',\n",
       " '<start> amour <end>',\n",
       " '<start> i don t mind <end>',\n",
       " '<start> where they at ? <end>',\n",
       " '<start> put em up ! <end>',\n",
       " '<start> where they at ? <end>',\n",
       " '<start> where they at ? <end>',\n",
       " '<start> put em up ! <end>',\n",
       " '<start> where they at ? <end>',\n",
       " '<start> where they at ? <end>',\n",
       " '<start> put em up ! <end>',\n",
       " '<start> put em up ! <end>',\n",
       " '<start> where they at ? <end>',\n",
       " '<start> where they at ? <end>',\n",
       " '<start> put em up ! <end>',\n",
       " '<start> where they at ? <end>',\n",
       " '<start> i m on my way <end>',\n",
       " '<start> i m on my way <end>',\n",
       " '<start> oh , oh , oh , ooh <end>',\n",
       " '<start> i m on my way <end>']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    \n",
    "    def count_character(data): \n",
    "        count = 0   \n",
    "        for i in data :  \n",
    "            count += len(i)\n",
    "        return  count\n",
    "    sentence_counter = sentence\n",
    "    sentence_counter_data = sentence_counter.split() \n",
    "    #sentence_count = len(sentence_data)\n",
    " \n",
    "    if count_character(sentence_counter_data) < 13:\n",
    "        #print(\"공백을 제외한 문자수 : \",count_character(sentence_data)) \n",
    "        sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "        return sentence\n",
    "    else :\n",
    "        return False\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요. \n",
    "\n",
    "\n",
    "\n",
    "# <end>를 없애면 소스 문장, 첫 단어 <start>를 없애면 타겟 문장\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    if preprocess_sentence(sentence) == False : continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "color-disaster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 1257 1760 ...    0    0    0]\n",
      " [   2 1761   21 ...    0    0    0]\n",
      " [   2 1763   23 ...    0    0    0]\n",
      " ...\n",
      " [   2    3    0 ...    0    0    0]\n",
      " [   2    3    0 ...    0    0    0]\n",
      " [   2    3    0 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f26c47f4890>\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "    \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    #tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    #for token in tensor:\n",
    "    #if len(tensor)>15:continue\n",
    "    #else:\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    \n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "print(len(tensor[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "sitting-slovakia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 1257 1760    3    0    0    0    0    0    0]\n",
      " [   2 1761   21 1762    3    0    0    0    0    0]\n",
      " [   2 1763   23   11 1015    3    0    0    0    0]]\n",
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : .\n",
      "7 : you\n",
      "8 : oh\n",
      "9 : it\n",
      "10 : me\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "challenging-africa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2 1257 1760    3    0    0    0    0    0    0    0    0]\n",
      "[1257 1760    3    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "# ## :, : ?의미를 확실히 알고가자\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "outer-addiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (11213, 12)\n",
      "Target Train: (11213, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(tgt_input, src_input, \n",
    "                                                                test_size=0.2, \n",
    "                                                                random_state = 1993, \n",
    "                                                                shuffle=True)\n",
    "\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "blank-viking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 12), (256, 12)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "attempted-david",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "43/43 [==============================] - 5s 126ms/step - loss: 3.6475 - val_loss: 2.5632\n",
      "Epoch 2/15\n",
      "43/43 [==============================] - 5s 112ms/step - loss: 2.1925 - val_loss: 1.8122\n",
      "Epoch 3/15\n",
      "43/43 [==============================] - 5s 111ms/step - loss: 1.5096 - val_loss: 1.4589\n",
      "Epoch 4/15\n",
      "43/43 [==============================] - 5s 111ms/step - loss: 1.3232 - val_loss: 1.3343\n",
      "Epoch 5/15\n",
      "43/43 [==============================] - 5s 112ms/step - loss: 1.1786 - val_loss: 1.1870\n",
      "Epoch 6/15\n",
      "43/43 [==============================] - 5s 112ms/step - loss: 1.0382 - val_loss: 1.0623\n",
      "Epoch 7/15\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.9010 - val_loss: 0.9200\n",
      "Epoch 8/15\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.7743 - val_loss: 0.8162\n",
      "Epoch 9/15\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.6719 - val_loss: 0.7396\n",
      "Epoch 10/15\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.5882 - val_loss: 0.6623\n",
      "Epoch 11/15\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.5123 - val_loss: 0.6082\n",
      "Epoch 12/15\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.4514 - val_loss: 0.5547\n",
      "Epoch 13/15\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.3898 - val_loss: 0.5137\n",
      "Epoch 14/15\n",
      "43/43 [==============================] - 5s 120ms/step - loss: 0.3404 - val_loss: 0.4725\n",
      "Epoch 15/15\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.2969 - val_loss: 0.4392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f27c023c3d0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
    "val_dataset = val_dataset.batch(256)\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=15, validation_data=val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "saving-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "coupled-adaptation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> you <start> you miss you miss you miss you miss you miss you miss you miss you miss you '"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start>love you\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-difficulty",
   "metadata": {},
   "source": [
    "1. 가사 텍스트 생성 모델이 정상적으로 동작하는가?\n",
    "텍스트 제너레이션 결과가 그럴듯한 문장으로 생성되는가?\n",
    "2. 데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었는가?\n",
    "특수문자 제거, 토크나이저 생성, 패딩처리 등의 과정이 빠짐없이 진행되었는가?\n",
    "3. 텍스트 생성모델이 안정적으로 학습되었는가?\n",
    "텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-broadcasting",
   "metadata": {},
   "source": [
    "\n",
    "# 결론\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-broadcasting",
   "metadata": {},
   "source": [
    "데이터 15단어 이하로 맞추기 위해서 <start>, <end> 를 삭제하지 않고 문장 내에서 \n",
    "단어를 카운팅 하여 줄이는게 핵심이었다. \n",
    "모델의 성능은 2.2이하로 만족할만한 수준이긴 하지만 작사 수준은 성능에 비해 많이 떨어지는듯하다\n",
    "데이터 단어 수를 좀 더 높여서 학습한다면 만족할만한 작사수준이 나오지 않을까 예상해본다 .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-struggle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
