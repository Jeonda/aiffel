{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [E4] 4th project NAVER Movie Review\n",
    "1. 네이버리뷰\n",
    " - 데이터 전처리\n",
    " - 데이터중복확인 및 제거\n",
    " - 토큰화\n",
    " - 정수 인코딩\n",
    " - 빈샘플 제거\n",
    " - 패딩\n",
    " - LSTM (RNN)\n",
    " - LSTM (RNN)\n",
    " - CNN\n",
    " - MaxPooling\n",
    "2. gensim Word2Vec 불러오기 \n",
    "3. Word2Vec 학습하기\n",
    "4. 결론\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 네이버 리뷰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지를 불러오기\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트용 리뷰 개수 : 50000\n",
      "훈련용 리뷰 개수 : 150000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터를 읽어봅시다. 네이버리뷰 미리 다운받은 데이터\n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/ratings_test.txt')\n",
    "\n",
    "print('테스트용 리뷰 개수 :',len(test_data)) # 테스트용 리뷰 개수 출력\n",
    "print('훈련용 리뷰 개수 :',len(train_data)) # 훈련용 리뷰 개수 출력\n",
    "\n",
    "# 데이터를 몇개만 출력해보자\n",
    "train_data.head() # 훈련용 데이터 헤드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146182, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련용 데이터 document와 label 개수를 호가인해보자.\n",
    "train_data['document'].nunique(), train_data['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 수 : 146183\n"
     ]
    }
   ],
   "source": [
    "# 데이터 중복 확인\n",
    "#훈련용 데이터 150000 개 중에 146182를 뺀 3818개가 document에서 중복\n",
    "#훈련용 데이터 labeL 0과 1만 가지므로 2가 출력됨. \n",
    "train_data.drop_duplicates(subset=['document'], inplace=True) \n",
    "print('총 샘플의 수 :',len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "id          0\n",
      "document    1\n",
      "label       0\n",
      "dtype: int64\n",
      "False\n",
      "146182\n",
      "id          0\n",
      "document    0\n",
      "label       0\n",
      "dtype: int64\n",
      "146182\n"
     ]
    }
   ],
   "source": [
    "# 데이터 중복 제거\n",
    "#document 열에서 중복인 내용 제거 후 총 샘플 수 확인\n",
    "# null값 확인\n",
    "print(train_data.isnull().values.any())\n",
    "print(train_data.isnull().sum())\n",
    "train_data.loc[train_data.document.isnull()]\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인\n",
    "print(len(train_data))\n",
    "\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "print(train_data.isnull().sum())\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 솔직히 재미는 없다평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                  아 더빙 진짜 짜증나네요 목소리      0\n",
       "1   3819312                         흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                          교도소 이야기구먼 솔직히 재미는 없다평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...      1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "# 한글과 공백을 제외하고 모두 제거\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 테스트용 샘플의 개수 : 48995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터도 동일하게 제거를 하자\n",
    "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 개수: 143682, 테스트 개수: 48417\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)\n",
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 빈샘플 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143652\n",
      "143652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
    "\n",
    "# 빈 샘플들을 제거\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "\n",
    "words = np.concatenate(X_train).tolist()\n",
    "counter = Counter(words)\n",
    "counter = counter.most_common(10000-4)    \n",
    "vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)} # enumerate 열거하다\n",
    "    \n",
    "def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 83\n",
      "리뷰의 평균 길이 : 13.937515662851892\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 30 이하인 샘플의 비율: 90.57235541447388\n"
     ]
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "plt.hist([len(s) for s in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))\n",
    "\n",
    "max_len = 30\n",
    "below_threshold_len(max_len, X_train)\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='pre', maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, padding='pre', maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         1000000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,117,377\n",
      "Trainable params: 1,117,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1916/1916 [==============================] - 12s 6ms/step - loss: 0.3913 - accuracy: 0.8204 - val_loss: 0.3544 - val_accuracy: 0.8431\n",
      "1514/1514 - 2s - loss: 0.3573 - accuracy: 0.8415\n",
      "[0.35729581117630005, 0.8415432572364807]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000  # 어휘 사전의 크기입니다(10개의 단어), 위에서 사이즈가 정해져 있으므로 생략\n",
    "word_vector_dim = 100  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(Embedding(vocab_size, word_vector_dim))\n",
    "model_LSTM.add(LSTM(128))\n",
    "model_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "model_LSTM.summary()\n",
    "\n",
    "epochs=1\n",
    "\n",
    "model_LSTM.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# validation 0.2 (20%설정)\n",
    "history = model_LSTM.fit(X_train, y_train, epochs=epochs, batch_size=60, validation_split=0.2)\n",
    "\n",
    "results = model_LSTM.evaluate(X_test,  y_test, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/ko/word2vec_ko_LSTM.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "#vocab_size\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model_LSTM.get_weights()[0]\n",
    "for i in range(4,vocab_size): #vocab_size\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word '사랑' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-f1950ea15956>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"사랑\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilar_by_word\u001b[0;34m(self, word, topn, restrict_vocab)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \"\"\"\n\u001b[0;32m--> 596\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilar_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '사랑' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "import gensim\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "word_vectors.similar_by_word(\"사랑\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전학습 데이터 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('슬픔', 0.7216663360595703),\n",
       " ('행복', 0.6759077310562134),\n",
       " ('절망', 0.6468985676765442),\n",
       " ('기쁨', 0.6458414793014526),\n",
       " ('이별', 0.6334798336029053),\n",
       " ('추억', 0.6320937871932983),\n",
       " ('인생', 0.6216273307800293),\n",
       " ('애정', 0.6206068992614746),\n",
       " ('연인', 0.6186063289642334),\n",
       " ('유혹', 0.5965287685394287)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "import gensim\n",
    "\n",
    "ko_model = gensim.models.Word2Vec.load('~/aiffel/ko/ko.bin')\n",
    "word2vec = ko_model\n",
    "word2vec.similar_by_word(\"사랑\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = Tokenizer()\n",
    "#tokenizer.fit_on_texts(X_train)\n",
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
    "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)\n",
    "      \n",
    "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
    "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "print(X_train[:3])\n",
    "\n",
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반적으로 자음의 범위는 ㄱ ~ ㅎ, 모음의 범위는 ㅏ ~ ㅣ와 같이 지정할 수 있습니다. 해당 범위 내에 어떤 자음과 모음이 속하는지 알고 싶다면 아래의 링크를 참고하시기 바랍니다.\n",
    "\n",
    "https://www.unicode.org/charts/PDF/U3130.pdf\n",
    "ㄱ ~ ㅎ: 3131 ~ 314E\n",
    "ㅏ ~ ㅣ: 314F ~ 3163\n",
    "\n",
    "또한 완성형 한글의 범위는 가 ~ 힣과 같이 사용합니다. 해당 범위 내에 포함된 음절들은 아래의 링크에서 확인할 수 있습니다.\n",
    "https://www.unicode.org/charts/PDF/UAC00.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "okt = Okt()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#데이터 중복 제거\n",
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = []\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)\n",
    "\n",
    "X_test = []\n",
    "for sentence in test_data['document']:\n",
    "    temp_X = []\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(temp_X)    \n",
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정수 인코딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
    "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
    "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)\n",
    "\n",
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "print(X_train[:3])\n",
    "\n",
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 빈샘플 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 샘플들을 제거\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.concatenate(X_train).tolist()\n",
    "counter = Counter(words)\n",
    "counter = counter.most_common(10000-4)    \n",
    "vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)} # enumerate 열거하다\n",
    "    \n",
    "def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 다른 모델 테스트용으로 데이터 가공이 달라질 수 있어서 미리 복사를 해두자\n",
    "\n",
    "X_train_2 = X_train\n",
    "X_test_2 = X_test\n",
    "y_train_2 = y_train\n",
    "y_test_2 = y_test\n",
    "\n",
    "X_train_3 = X_train\n",
    "X_test_3 = X_test\n",
    "y_train_3 = y_train\n",
    "y_test_3 = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "plt.hist([len(s) for s in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30\n",
    "below_threshold_len(max_len, X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30을 기준으로 패딩을 함. padding은 post보다 pre가 10% 가량 성능향상이 있으므로 pre로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, padding='pre', maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, padding='pre', maxlen = max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM으로 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(Embedding(vocab_size, 200))\n",
    "model_LSTM.add(LSTM(128))\n",
    "model_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "#mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "\n",
    "model_LSTM.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model_LSTM.fit(X_train, y_train, epochs=epochs, batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_LSTM.evaluate(X_test,  y_test, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oaded_model = load_model('best_model.h5')\n",
    "#print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_LSTM = model_LSTM.layers[0]\n",
    "weights = embedding_layer_LSTM.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_dim = 200  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다.\n",
    "word2vec_file_path_LSTM = os.getenv('HOME')+'/aiffel/ko/kor_w2v.txt'\n",
    "f = open(word2vec_file_path_LSTM, 'w')\n",
    "f.write('{} {}\\n'.format(10000-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors_LSTM = model_LSTM.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4,10000):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors_LSTM[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_LSTM = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path_LSTM, binary=False)\n",
    "word_vectors_LSTM.similar_by_word(\"사랑\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "for i in range(4,10000):\n",
    "    if index_to_word[i] in word_vectors_LSTM:\n",
    "        embedding_matrix[i] = word_vectors_LSTM[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(keras.layers.Embedding(10000, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=max_len, \n",
    "                                 trainable=False))   # trainable을 True로 주면 Fine-tuningmodel_LSTM.add(LSTM(128))\n",
    "model_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size = 100  # 어휘 사전의 크기입니다(10개의 단어), 위에서 사이즈가 정해져 있으므로 생략\n",
    "\n",
    "word_vector_dim = 100  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "# 모델은 위와 구조는 동일하나 노드에서 나온 모델로 다시 테스트를 해보자. \n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경가능)\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train_2))\n",
    "print(len(y_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보통 20프로 정도 validation데이터로 사용하므로 3만건 정도 생각함!\n",
    "# validation set 30000건 분리\n",
    "\n",
    "x_val = X_train_2[:30000]   \n",
    "y_val = y_train_2[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set을 제외한 나머지 115380건\n",
    "partial_x_train = X_train_2[30000:]  \n",
    "partial_y_train = y_train_2[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30\n",
    "below_threshold_len(max_len, X_train_2)\n",
    "\n",
    "#partial_x_train = pad_sequences(partial_x_train, maxlen = max_len)\n",
    "partial_x_train = pad_sequences(partial_x_train, maxlen = max_len)\n",
    "X_test_2 = pad_sequences(X_test_2, maxlen = max_len)\n",
    "x_val = pad_sequences(x_val, maxlen = max_len)\n",
    "#y_val = pad_sequences(y_val, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test_2,  y_test_2, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test_2, y_test_2)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = X_train_3[:30000]   \n",
    "y_val = y_train_3[:30000]\n",
    "partial_x_train = X_train_3[30000:]  \n",
    "partial_y_train = y_train_3[30000:]\n",
    "max_len = 30\n",
    "below_threshold_len(max_len, X_train_3)\n",
    "\n",
    "#partial_x_train = pad_sequences(partial_x_train, maxlen = max_len)\n",
    "partial_x_train = pad_sequences(partial_x_train, maxlen = max_len)\n",
    "X_test_3 = pad_sequences(X_test_3, maxlen = max_len)\n",
    "x_val = pad_sequences(x_val, maxlen = max_len)\n",
    "#y_val = pad_sequences(y_val, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_dim = 16   # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test_3,  y_test_3, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxpooling 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test_3,  y_test_3, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. gensim, Word2Vec 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import gensim\n",
    "from konlpy.tag import Okt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_model = gensim.models.Word2Vec.load('~/aiffel/ko/ko.bin')\n",
    "a = ko_model.wv.most_similar(\"사랑\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Word2Vec 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('~/aiffel/ko/ratings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NULL 값 존재 유무\n",
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data)) # 리뷰 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규 표현식을 통한 한글 외 문자 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:5] # 상위 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 정의\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 OKT를 사용한 토큰화 작업 (다소 시간 소요)\n",
    "okt = Okt()\n",
    "tokenized_data = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    tokenized_data.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰 길이 분포 확인\n",
    "print('리뷰의 최대 길이 :',max(len(l) for l in tokenized_data))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))\n",
    "plt.hist([len(s) for s in tokenized_data], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences = tokenized_data, size = 100, window = 5, min_count = 5, workers = 4, sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성된 임베딩 매트릭스의 크기 확인\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(\"사랑\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model.wv.save_word2vec_format('kor_w2v') # 모델 저장\n",
    "loaded_model \n",
    "= KeyedVectors.load_word2vec_format(\"kor_w2v\") # 모델 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = loaded_model.most_similar(\"사랑\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습된 word2vec 로 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/ko/kor_w2v.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "\n",
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(index_to_word[1])     # 'the' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 1 이 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['사랑']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in ko_model:\n",
    "        embedding_matrix[i] = ko_model[index_to_word[i]]\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()\n",
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)\n",
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 결론\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어 처리에서 RNN, CNN, Maxpooling 모델을 통해서 결과를 확인해보았다.\n",
    "결과는 예상대로 RNN이 가장 높은 성능을 보여주었고 CNN의 경우는 다소 학습이 안되는 결과로 보여졌다.\n",
    "  LSTM 1 - loss: 0.3451 - acc: 0.8549\n",
    "  LSTM 2 - loss: 0.8651 - acc: 0.8183\n",
    "  CNN - loss: 0.8807 - acc: 0.5724\n",
    "  Maxpooling - loss: 0.4654 - acc: 0.8101\n",
    "LSTM은 Long Short-Term Memory는 RNN에서 forget gate를 추가한것으로 RNN의 일종으로 모델을 구성하였다.\n",
    "자연어처리에서 전단계의 과거 학습 데이터를 재귀 시키는 형태로서, 이전의 값을 메모리 형태로 기억되어 순차적인 데이터열 처리에 적합함을 확인할 수 있었다.\n",
    "CNN에서 1-D conv층으로 RNN과 같이 자연어 처리를 할 수 있지만 이전 데이터가 중요한 자연어처리에서 다소 모델이 안좋게 나온것을 확인할 수 있었다. 노드에서 추천해준대로 RNN, CNN을 결합하여 하이브리드 형태로 구성을 하면 더 좋은 성능의 모델을 구성할 수 있을것으로 예상을 한다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
